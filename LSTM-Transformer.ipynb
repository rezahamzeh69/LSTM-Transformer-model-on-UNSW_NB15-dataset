{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SPfOBDXvIJgk",
        "outputId": "8aead033-ddb3-43a0-e849-97e6ef9fd436"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading UNSW-NB15 dataset using kagglehub...\n",
            "Dataset downloaded to: /kaggle/input/unswnb15\n",
            "Attempting to load training data from: /kaggle/input/unswnb15/UNSW_NB15_training-set.parquet\n",
            "Attempting to load testing data from: /kaggle/input/unswnb15/UNSW_NB15_testing-set.parquet\n",
            "Datasets loaded successfully.\n",
            "Preprocessing datasets...\n",
            "Preprocessing finished. Input dimension: 188\n",
            "Creating sequences...\n",
            "Raw training sequences: 175327\n",
            "Raw testing sequences: 82318\n",
            "Splitting 15.0% of training sequences for validation.\n",
            "Final Train sequences: 149027\n",
            "Validation sequences: 26300\n",
            "Final Test sequences: 82318\n",
            "Using class weights for CrossEntropyLoss: [2.101370e-05 9.858141e-06]\n",
            "\n",
            "--- Starting Training on cuda ---\n",
            "Epoch 1/30 | Train Loss: 0.1919 | Val Loss: 0.1418 | Val Acc: 0.9433 | Val F1: 0.9579 | Time: 10.69s\n",
            "Epoch 2/30 | Train Loss: 0.1309 | Val Loss: 0.0924 | Val Acc: 0.9640 | Val F1: 0.9734 | Time: 9.82s\n",
            "Epoch 3/30 | Train Loss: 0.0942 | Val Loss: 0.0796 | Val Acc: 0.9700 | Val F1: 0.9778 | Time: 9.70s\n",
            "Epoch 4/30 | Train Loss: 0.0852 | Val Loss: 0.0751 | Val Acc: 0.9737 | Val F1: 0.9807 | Time: 9.79s\n",
            "Epoch 5/30 | Train Loss: 0.0796 | Val Loss: 0.0750 | Val Acc: 0.9683 | Val F1: 0.9764 | Time: 9.90s\n",
            "Epoch 6/30 | Train Loss: 0.0763 | Val Loss: 0.0734 | Val Acc: 0.9735 | Val F1: 0.9805 | Time: 9.84s\n",
            "Epoch 7/30 | Train Loss: 0.0738 | Val Loss: 0.0713 | Val Acc: 0.9723 | Val F1: 0.9795 | Time: 10.06s\n",
            "Epoch 8/30 | Train Loss: 0.0711 | Val Loss: 0.0665 | Val Acc: 0.9733 | Val F1: 0.9803 | Time: 9.96s\n",
            "Epoch 9/30 | Train Loss: 0.0702 | Val Loss: 0.0670 | Val Acc: 0.9712 | Val F1: 0.9786 | Time: 10.01s\n",
            "Epoch 10/30 | Train Loss: 0.0667 | Val Loss: 0.0665 | Val Acc: 0.9762 | Val F1: 0.9824 | Time: 10.23s\n",
            "Epoch 11/30 | Train Loss: 0.0658 | Val Loss: 0.0723 | Val Acc: 0.9749 | Val F1: 0.9816 | Time: 10.23s\n",
            "Epoch 12/30 | Train Loss: 0.0635 | Val Loss: 0.0713 | Val Acc: 0.9776 | Val F1: 0.9836 | Time: 10.25s\n",
            "Epoch 13/30 | Train Loss: 0.0617 | Val Loss: 0.0681 | Val Acc: 0.9770 | Val F1: 0.9831 | Time: 10.12s\n",
            "Early stopping triggered after epoch 13!\n",
            "--- Training Finished --- Total time: 2m 11s\n",
            "Loaded best model weights for final evaluation.\n",
            "\n",
            "--- Final Test Set Evaluation ---\n",
            "Test Loss: 0.0343\n",
            "Test Accuracy: 0.9887\n",
            "Test F1 Score (Binary): 0.9896\n",
            "\n",
            "Confusion Matrix:\n",
            "[[36783   203]\n",
            " [  731 44601]]\n",
            "\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "  Normal (0)       0.98      0.99      0.99     36986\n",
            "  Attack (1)       1.00      0.98      0.99     45332\n",
            "\n",
            "    accuracy                           0.99     82318\n",
            "   macro avg       0.99      0.99      0.99     82318\n",
            "weighted avg       0.99      0.99      0.99     82318\n",
            "\n",
            "--------------------------------\n",
            "\n",
            "Execution finished successfully.\n"
          ]
        }
      ],
      "source": [
        "# Install pyarrow if necessary (usually pre-installed in Colab)\n",
        "# !pip install pyarrow\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import MinMaxScaler, LabelEncoder\n",
        "from sklearn.metrics import f1_score, classification_report, confusion_matrix\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import time\n",
        "import os\n",
        "import kagglehub\n",
        "import io\n",
        "\n",
        "LSTM_HIDDEN_DIM = 128\n",
        "LSTM_LAYERS = 2\n",
        "TRANSFORMER_DIM = 128\n",
        "NHEAD = 8\n",
        "NUM_TRANSFORMER_LAYERS = 2\n",
        "NUM_CLASSES = 2\n",
        "DROPOUT = 0.3\n",
        "SEQUENCE_LENGTH = 15\n",
        "BATCH_SIZE = 256\n",
        "LEARNING_RATE = 5e-4\n",
        "NUM_EPOCHS = 30\n",
        "VALIDATION_SPLIT_FROM_TRAIN = 0.15\n",
        "RANDOM_SEED = 42\n",
        "EARLY_STOPPING_PATIENCE = 5\n",
        "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "TRAIN_FILE_NAME = 'UNSW_NB15_training-set.parquet'\n",
        "TEST_FILE_NAME = 'UNSW_NB15_testing-set.parquet'\n",
        "\n",
        "torch.manual_seed(RANDOM_SEED)\n",
        "np.random.seed(RANDOM_SEED)\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.manual_seed_all(RANDOM_SEED)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "\n",
        "def load_and_preprocess_unsw(dataset_dir):\n",
        "    train_file_path = os.path.join(dataset_dir, TRAIN_FILE_NAME)\n",
        "    test_file_path = os.path.join(dataset_dir, TEST_FILE_NAME)\n",
        "    print(f\"Attempting to load training data from: {train_file_path}\")\n",
        "    print(f\"Attempting to load testing data from: {test_file_path}\")\n",
        "\n",
        "    if not os.path.exists(train_file_path) or not os.path.exists(test_file_path):\n",
        "         print(\"Error: Training or Testing file not found in the downloaded dataset directory.\")\n",
        "         print(f\"Contents of {dataset_dir}: {os.listdir(dataset_dir)}\")\n",
        "         return None, None, None, None, -1\n",
        "\n",
        "    try:\n",
        "        df_train = pd.read_parquet(train_file_path)\n",
        "        df_test = pd.read_parquet(test_file_path)\n",
        "        print(\"Datasets loaded successfully.\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading Parquet files: {e}\")\n",
        "        return None, None, None, None, -1\n",
        "\n",
        "    print(\"Preprocessing datasets...\")\n",
        "    df = pd.concat([df_train, df_test], ignore_index=True)\n",
        "    df = df.drop(['id', 'attack_cat'], axis=1, errors='ignore')\n",
        "    df.columns = df.columns.str.strip().str.lower().str.replace(' ', '_').str.replace(r'[^a-z0-9_]', '', regex=True)\n",
        "\n",
        "    categorical_cols = df.select_dtypes(include='object').columns.tolist()\n",
        "    categorical_cols.extend(df.select_dtypes(include='category').columns.tolist())\n",
        "    categorical_cols = list(set(categorical_cols))\n",
        "\n",
        "    numerical_cols = df.select_dtypes(include=np.number).columns.tolist()\n",
        "\n",
        "    for col in numerical_cols:\n",
        "        if df[col].isnull().any():\n",
        "            df[col] = df[col].fillna(df[col].median() if df[col].nunique() > 10 else 0)\n",
        "    for col in categorical_cols:\n",
        "        if df[col].isnull().any():\n",
        "            if pd.api.types.is_categorical_dtype(df[col]):\n",
        "                df[col] = df[col].astype('object')\n",
        "            df[col] = df[col].fillna(df[col].mode()[0])\n",
        "\n",
        "    if 'label' in numerical_cols:\n",
        "         numerical_cols.remove('label')\n",
        "    elif 'label' in categorical_cols:\n",
        "        categorical_cols.remove('label')\n",
        "        try:\n",
        "            df['label'] = pd.to_numeric(df['label'])\n",
        "        except ValueError:\n",
        "            print(\"Error: Could not convert 'label' column to numeric.\")\n",
        "            return None, None, None, None, -1\n",
        "\n",
        "    if 'label' not in df.columns or not pd.api.types.is_numeric_dtype(df['label']):\n",
        "         print(\"Error: 'label' column is missing or not numeric after cleaning.\")\n",
        "         # Optionally print dtypes to help debug\n",
        "         # print(df.info())\n",
        "         return None, None, None, None, -1\n",
        "\n",
        "    df_encoded = pd.get_dummies(df, columns=categorical_cols, dummy_na=False)\n",
        "\n",
        "    train_len = len(df_train)\n",
        "    df_train_processed = df_encoded.iloc[:train_len].copy()\n",
        "    df_test_processed = df_encoded.iloc[train_len:].copy()\n",
        "\n",
        "    scaler = MinMaxScaler()\n",
        "    # Important: Get numerical columns *after* encoding, exclude label\n",
        "    numerical_cols_encoded = df_train_processed.drop('label', axis=1).select_dtypes(include=np.number).columns.tolist()\n",
        "\n",
        "    if numerical_cols_encoded:\n",
        "        # Ensure columns are float32 *before* scaling to potentially avoid some warnings and ensure consistency\n",
        "        for col in numerical_cols_encoded:\n",
        "             df_train_processed[col] = df_train_processed[col].astype(np.float32)\n",
        "             df_test_processed[col] = df_test_processed[col].astype(np.float32)\n",
        "\n",
        "        scaler.fit(df_train_processed[numerical_cols_encoded])\n",
        "        # Use .loc to assign back safely\n",
        "        df_train_processed.loc[:, numerical_cols_encoded] = scaler.transform(df_train_processed[numerical_cols_encoded])\n",
        "        df_test_processed.loc[:, numerical_cols_encoded] = scaler.transform(df_test_processed[numerical_cols_encoded])\n",
        "\n",
        "    # --- Critical Change Here ---\n",
        "    # Explicitly convert all feature columns to float32 before .values\n",
        "    feature_cols = df_train_processed.columns.drop('label')\n",
        "    try:\n",
        "        X_train = df_train_processed[feature_cols].astype(np.float32).values\n",
        "        X_test = df_test_processed[feature_cols].astype(np.float32).values\n",
        "    except Exception as e:\n",
        "        print(f\"Error converting feature columns to float32 before .values: {e}\")\n",
        "        # Identify problematic columns:\n",
        "        for col in feature_cols:\n",
        "            try:\n",
        "                df_train_processed[col].astype(np.float32)\n",
        "            except Exception as col_e:\n",
        "                print(f\"  - Column '{col}' failed conversion: {col_e}, dtype: {df_train_processed[col].dtype}\")\n",
        "        return None, None, None, None, -1\n",
        "\n",
        "    # Ensure labels are of a type compatible with long tensors later\n",
        "    y_train = df_train_processed['label'].astype(np.int64).values\n",
        "    y_test = df_test_processed['label'].astype(np.int64).values\n",
        "\n",
        "    input_dim = X_train.shape[1]\n",
        "    print(f\"Preprocessing finished. Input dimension: {input_dim}\")\n",
        "    return X_train, y_train, X_test, y_test, input_dim\n",
        "\n",
        "\n",
        "def create_sequences(features, labels, sequence_length):\n",
        "    sequences = []\n",
        "    sequence_labels = []\n",
        "    if len(features) < sequence_length:\n",
        "        return np.array([]), np.array([])\n",
        "    for i in range(len(features) - sequence_length + 1):\n",
        "        sequences.append(features[i:i+sequence_length])\n",
        "        sequence_labels.append(labels[i+sequence_length-1])\n",
        "    # Ensure output arrays are float32 and int64 respectively\n",
        "    return np.array(sequences, dtype=np.float32), np.array(sequence_labels, dtype=np.int64)\n",
        "\n",
        "\n",
        "class LSTMTransformerModel(nn.Module):\n",
        "    def __init__(self, input_dim, lstm_hidden_dim, lstm_layers,\n",
        "                 transformer_dim, nhead, num_transformer_layers,\n",
        "                 num_classes, dropout):\n",
        "        super(LSTMTransformerModel, self).__init__()\n",
        "        self.lstm = nn.LSTM(input_dim, lstm_hidden_dim,\n",
        "                            num_layers=lstm_layers, batch_first=True,\n",
        "                            dropout=dropout if lstm_layers > 1 else 0,\n",
        "                            bidirectional=False)\n",
        "        self.fc_lstm_transformer = nn.Linear(lstm_hidden_dim, transformer_dim)\n",
        "        encoder_layer = nn.TransformerEncoderLayer(d_model=transformer_dim,\n",
        "                                                   nhead=nhead,\n",
        "                                                   dim_feedforward=transformer_dim * 4,\n",
        "                                                   dropout=dropout,\n",
        "                                                   activation='gelu',\n",
        "                                                   batch_first=True)\n",
        "        self.transformer_encoder = nn.TransformerEncoder(encoder_layer,\n",
        "                                                         num_layers=num_transformer_layers)\n",
        "        self.global_pool = nn.AdaptiveAvgPool1d(1)\n",
        "        self.dropout_layer = nn.Dropout(dropout)\n",
        "        self.classifier = nn.Linear(transformer_dim, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        lstm_out, _ = self.lstm(x)\n",
        "        transformer_input = self.fc_lstm_transformer(lstm_out)\n",
        "        transformer_out = self.transformer_encoder(transformer_input)\n",
        "        pooled = self.global_pool(transformer_out.transpose(1, 2)).squeeze(-1)\n",
        "        dropped_out = self.dropout_layer(pooled)\n",
        "        logits = self.classifier(dropped_out)\n",
        "        return logits\n",
        "\n",
        "\n",
        "def train_model(model, train_loader, val_loader, criterion, optimizer, num_epochs, device, patience):\n",
        "    history = {'train_loss': [], 'val_loss': [], 'val_accuracy': [], 'val_f1_score': []}\n",
        "    best_val_loss = float('inf')\n",
        "    epochs_no_improve = 0\n",
        "    print(f\"\\n--- Starting Training on {device} ---\")\n",
        "    total_start_time = time.time()\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        epoch_start_time = time.time()\n",
        "        model.train()\n",
        "        running_loss = 0.0\n",
        "        for batch_data, batch_labels in train_loader:\n",
        "            batch_data, batch_labels = batch_data.to(device), batch_labels.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(batch_data)\n",
        "            loss = criterion(outputs, batch_labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            running_loss += loss.item() * batch_data.size(0)\n",
        "\n",
        "        train_loss = running_loss / len(train_loader.dataset)\n",
        "        # Evaluate on validation set if val_loader exists\n",
        "        if val_loader:\n",
        "             val_loss, val_accuracy, val_f1_score = evaluate_model(model, val_loader, criterion, device)\n",
        "             history['train_loss'].append(train_loss)\n",
        "             history['val_loss'].append(val_loss)\n",
        "             history['val_accuracy'].append(val_accuracy)\n",
        "             history['val_f1_score'].append(val_f1_score)\n",
        "             epoch_duration = time.time() - epoch_start_time\n",
        "             print(f\"Epoch {epoch+1}/{num_epochs} | Train Loss: {train_loss:.4f} | Val Loss: {val_loss:.4f} | Val Acc: {val_accuracy:.4f} | Val F1: {val_f1_score:.4f} | Time: {epoch_duration:.2f}s\")\n",
        "\n",
        "             if val_loss < best_val_loss:\n",
        "                 best_val_loss = val_loss\n",
        "                 epochs_no_improve = 0\n",
        "                 torch.save(model.state_dict(), 'best_lstm_transformer_model.pth')\n",
        "             else:\n",
        "                 epochs_no_improve += 1\n",
        "                 if epochs_no_improve >= patience:\n",
        "                     print(f\"Early stopping triggered after epoch {epoch+1}!\")\n",
        "                     break\n",
        "        else: # No validation loader\n",
        "             history['train_loss'].append(train_loss)\n",
        "             history['val_loss'].append(np.nan) # Placeholder\n",
        "             history['val_accuracy'].append(np.nan)\n",
        "             history['val_f1_score'].append(np.nan)\n",
        "             epoch_duration = time.time() - epoch_start_time\n",
        "             print(f\"Epoch {epoch+1}/{num_epochs} | Train Loss: {train_loss:.4f} | Time: {epoch_duration:.2f}s\")\n",
        "\n",
        "\n",
        "    total_training_time = time.time() - total_start_time\n",
        "    print(f\"--- Training Finished --- Total time: {total_training_time // 60:.0f}m {total_training_time % 60:.0f}s\")\n",
        "\n",
        "    # Load best model if validation was performed and model was saved\n",
        "    if val_loader and os.path.exists('best_lstm_transformer_model.pth'):\n",
        "        try:\n",
        "          model.load_state_dict(torch.load('best_lstm_transformer_model.pth'))\n",
        "          print(\"Loaded best model weights for final evaluation.\")\n",
        "        except Exception as e:\n",
        "          print(f\"Warning: Could not load best model weights ({e}). Using last epoch model.\")\n",
        "    elif not val_loader:\n",
        "         # If no validation, save the last epoch model explicitly\n",
        "         torch.save(model.state_dict(), 'last_epoch_lstm_transformer_model.pth')\n",
        "         print(\"Saved model from last epoch (no validation performed).\")\n",
        "\n",
        "    return history\n",
        "\n",
        "\n",
        "def evaluate_model(model, data_loader, criterion, device, is_test_set=False):\n",
        "    model.eval()\n",
        "    total_loss = 0.0\n",
        "    correct_predictions = 0\n",
        "    all_labels = []\n",
        "    all_predictions = []\n",
        "    if not data_loader or len(data_loader.dataset) == 0:\n",
        "        # Return NaNs or Zeros if called during training loop without val_loader\n",
        "        if not is_test_set: return np.nan, np.nan, np.nan\n",
        "        print(\"Warning: Cannot evaluate on empty or invalid dataloader.\")\n",
        "        return 0.0, 0.0, 0.0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch_data, batch_labels in data_loader:\n",
        "            batch_data, batch_labels = batch_data.to(device), batch_labels.to(device)\n",
        "            outputs = model(batch_data)\n",
        "            loss = criterion(outputs, batch_labels)\n",
        "            total_loss += loss.item() * batch_data.size(0)\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            correct_predictions += (predicted == batch_labels).sum().item()\n",
        "            all_labels.extend(batch_labels.cpu().numpy())\n",
        "            all_predictions.extend(predicted.cpu().numpy())\n",
        "\n",
        "    avg_loss = total_loss / len(data_loader.dataset)\n",
        "    accuracy = correct_predictions / len(data_loader.dataset)\n",
        "    # Ensure labels/predictions are not empty before calculating F1\n",
        "    f1 = 0.0\n",
        "    if len(all_labels) > 0 and len(all_predictions) > 0:\n",
        "         f1 = f1_score(all_labels, all_predictions, average='binary', zero_division=0)\n",
        "\n",
        "\n",
        "    if is_test_set:\n",
        "        print(\"\\n--- Final Test Set Evaluation ---\")\n",
        "        print(f\"Test Loss: {avg_loss:.4f}\")\n",
        "        print(f\"Test Accuracy: {accuracy:.4f}\")\n",
        "        print(f\"Test F1 Score (Binary): {f1:.4f}\")\n",
        "        if len(all_labels) > 0 and len(all_predictions) > 0:\n",
        "            print(\"\\nConfusion Matrix:\")\n",
        "            print(confusion_matrix(all_labels, all_predictions))\n",
        "            print(\"\\nClassification Report:\")\n",
        "            print(classification_report(all_labels, all_predictions, target_names=['Normal (0)', 'Attack (1)'], zero_division=0))\n",
        "        else:\n",
        "             print(\"\\nNo labels/predictions available for Confusion Matrix/Classification Report.\")\n",
        "        print(\"--------------------------------\")\n",
        "    return avg_loss, accuracy, f1\n",
        "\n",
        "\n",
        "# --- Main Execution ---\n",
        "print(\"Downloading UNSW-NB15 dataset using kagglehub...\")\n",
        "try:\n",
        "    dataset_path = kagglehub.dataset_download(\"dhoogla/unswnb15\")\n",
        "    print(f\"Dataset downloaded to: {dataset_path}\")\n",
        "except Exception as e:\n",
        "    print(f\"Error downloading dataset: {e}\")\n",
        "    print(\"Please ensure Kaggle API credentials are set up correctly in your environment.\")\n",
        "    dataset_path = None\n",
        "\n",
        "if dataset_path and os.path.isdir(dataset_path):\n",
        "    X_train_raw, y_train_raw, X_test_raw, y_test_raw, input_dim = load_and_preprocess_unsw(dataset_path)\n",
        "\n",
        "    if X_train_raw is not None and input_dim != -1 :\n",
        "        INPUT_DIM = input_dim\n",
        "        print(\"Creating sequences...\")\n",
        "        # Ensure create_sequences outputs correct dtypes now\n",
        "        X_train_seq, y_train_seq = create_sequences(X_train_raw, y_train_raw, SEQUENCE_LENGTH)\n",
        "        X_test_seq, y_test_seq = create_sequences(X_test_raw, y_test_raw, SEQUENCE_LENGTH)\n",
        "\n",
        "        if len(X_train_seq) == 0 or len(X_test_seq) == 0:\n",
        "             print(\"Error: Not enough data to create sequences with the specified length.\")\n",
        "        else:\n",
        "            print(f\"Raw training sequences: {len(X_train_seq)}\")\n",
        "            print(f\"Raw testing sequences: {len(X_test_seq)}\")\n",
        "\n",
        "            X_train_final_seq, X_val_seq, y_train_final_seq, y_val_seq = [], [], [], []\n",
        "            if len(X_train_seq) > 1 and VALIDATION_SPLIT_FROM_TRAIN > 0:\n",
        "                try:\n",
        "                    # Stratify requires labels (y_train_seq)\n",
        "                    X_train_final_seq, X_val_seq, y_train_final_seq, y_val_seq = train_test_split(\n",
        "                        X_train_seq, y_train_seq,\n",
        "                        test_size=VALIDATION_SPLIT_FROM_TRAIN,\n",
        "                        random_state=RANDOM_SEED, stratify=y_train_seq\n",
        "                    )\n",
        "                    print(f\"Splitting {VALIDATION_SPLIT_FROM_TRAIN*100:.1f}% of training sequences for validation.\")\n",
        "                except ValueError as e:\n",
        "                    print(f\"Warning: Could not stratify split ({e}). Performing non-stratified split.\")\n",
        "                    X_train_final_seq, X_val_seq, y_train_final_seq, y_val_seq = train_test_split(\n",
        "                        X_train_seq, y_train_seq, test_size=VALIDATION_SPLIT_FROM_TRAIN, random_state=RANDOM_SEED\n",
        "                    )\n",
        "                print(f\"Final Train sequences: {len(X_train_final_seq)}\")\n",
        "                print(f\"Validation sequences: {len(X_val_seq)}\")\n",
        "                print(f\"Final Test sequences: {len(X_test_seq)}\")\n",
        "\n",
        "            elif len(X_train_seq) > 0:\n",
        "                 print(\"Using all training sequences for training, no validation split performed.\")\n",
        "                 X_train_final_seq, y_train_final_seq = X_train_seq, y_train_seq\n",
        "                 X_val_seq, y_val_seq = np.array([]), np.array([])\n",
        "                 print(f\"Final Train sequences: {len(X_train_final_seq)}\")\n",
        "                 print(f\"Validation sequences: 0\")\n",
        "                 print(f\"Final Test sequences: {len(X_test_seq)}\")\n",
        "            else:\n",
        "                 print(\"Error: No training sequences available.\")\n",
        "                 X_train_final_seq = []\n",
        "\n",
        "\n",
        "            if len(X_train_final_seq) > 0:\n",
        "                # Convert NumPy arrays to Tensors - dtypes should be correct now\n",
        "                X_train_tensor = torch.from_numpy(X_train_final_seq) # dtype=float32 from create_sequences\n",
        "                y_train_tensor = torch.from_numpy(y_train_final_seq) # dtype=int64 from create_sequences\n",
        "                X_val_tensor = torch.from_numpy(X_val_seq)       # dtype=float32\n",
        "                y_val_tensor = torch.from_numpy(y_val_seq)       # dtype=int64\n",
        "                X_test_tensor = torch.from_numpy(X_test_seq)     # dtype=float32\n",
        "                y_test_tensor = torch.from_numpy(y_test_seq)     # dtype=int64\n",
        "\n",
        "\n",
        "                train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
        "                val_dataset = TensorDataset(X_val_tensor, y_val_tensor) if len(X_val_seq) > 0 else None\n",
        "                test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
        "\n",
        "                num_workers = 2 if DEVICE == 'cuda' else 0\n",
        "                pin_memory_flag = True if DEVICE == 'cuda' else False\n",
        "\n",
        "                train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=num_workers, pin_memory=pin_memory_flag, drop_last=True) # drop_last can help with batch norm layers if batch size varies\n",
        "                val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=num_workers, pin_memory=pin_memory_flag) if val_dataset else None\n",
        "                test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=num_workers, pin_memory=pin_memory_flag)\n",
        "\n",
        "                model = LSTMTransformerModel(INPUT_DIM, LSTM_HIDDEN_DIM, LSTM_LAYERS,\n",
        "                                             TRANSFORMER_DIM, NHEAD, NUM_TRANSFORMER_LAYERS,\n",
        "                                             NUM_CLASSES, DROPOUT).to(DEVICE)\n",
        "\n",
        "                # Check for class imbalance and apply weighting if desired\n",
        "                if len(y_train_final_seq) > 0:\n",
        "                    class_counts = np.bincount(y_train_final_seq)\n",
        "                    if len(class_counts) == NUM_CLASSES and 0 not in class_counts:\n",
        "                        class_weights = torch.tensor([1.0 / c for c in class_counts], dtype=torch.float32).to(DEVICE)\n",
        "                        criterion = nn.CrossEntropyLoss(weight=class_weights)\n",
        "                        print(f\"Using class weights for CrossEntropyLoss: {class_weights.cpu().numpy()}\")\n",
        "                    else:\n",
        "                        print(\"Using standard CrossEntropyLoss (no weighting or class count issue).\")\n",
        "                        criterion = nn.CrossEntropyLoss()\n",
        "                else:\n",
        "                     criterion = nn.CrossEntropyLoss() # Default if no training labels\n",
        "\n",
        "                optimizer = optim.AdamW(model.parameters(), lr=LEARNING_RATE, weight_decay=1e-5)\n",
        "\n",
        "                history = train_model(model, train_loader, val_loader, criterion, optimizer, NUM_EPOCHS, DEVICE, EARLY_STOPPING_PATIENCE)\n",
        "\n",
        "                evaluate_model(model, test_loader, criterion, DEVICE, is_test_set=True)\n",
        "                print(\"\\nExecution finished successfully.\")\n",
        "\n",
        "            else:\n",
        "                 print(\"\\nExecution aborted: No training data available after sequencing/splitting.\")\n",
        "\n",
        "    else:\n",
        "        print(\"\\nExecution aborted due to data loading/preprocessing errors.\")\n",
        "else:\n",
        "    print(\"\\nExecution aborted: Dataset download failed or directory not found.\")"
      ]
    }
  ]
}